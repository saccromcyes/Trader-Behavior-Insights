{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17456002-29b5-4b0f-9b4a-9d30c2a3c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes FG, trades: (2644, 6) (211224, 17)\n",
      "FG cleaned rows: 2644\n",
      "Candidate timestamp columns found: ['Timestamp IST', 'Timestamp', 'timestamp updated']\n",
      "→ Timestamp IST: 188 unique trading days\n",
      "→ Timestamp: 1 unique trading days\n",
      "→ timestamp updated: 7 unique trading days\n",
      "Best timestamp column selected: Timestamp IST (188 unique days)\n",
      "Trade timestamp range: 2023-04-30 18:30:00+00:00 → 2025-04-30 18:30:00+00:00\n",
      "FG timestamp range: 2018-02-01 05:30:00+00:00 → 2025-05-02 05:30:00+00:00\n",
      "Unique trade days detected: 480\n",
      "\n",
      "Merging each trade with nearest Fear-Greed timestamp (within 1 day)...\n",
      "Final merged rows: 211224\n",
      "Merged timestamp range: 2023-04-30 18:30:00+00:00 → 2025-04-30 18:30:00+00:00\n",
      "\n",
      "Sentiment summary:\n",
      "   classification  trades    avg_pnl  median_pnl   avg_volume      avg_exec\n",
      "0   Extreme Fear   21400  34.537862         0.0  5349.731843   7054.795108\n",
      "1  Extreme Greed   39992  67.892861         0.0  3112.251565   6082.195865\n",
      "2           Fear   61837  54.290400         0.0  7816.109931  14152.620222\n",
      "3          Greed   50309  43.582684         0.0  5737.962662  13409.677192\n",
      "4        Neutral   37686  34.307718         0.0  4782.732661  12393.692779\n",
      "\n",
      "Kruskal-Wallis p-value: 0.0000\n",
      "\n",
      "Random Forest RMSE: 517.9934\n",
      "Rows merged: 211224\n",
      "Correlation (Fear-Greed vs Closed_PnL): 0.0083\n",
      "Kruskal-Wallis p-value: 5.1417e-264\n",
      "Random Forest RMSE: 517.993\n",
      "Sentiment impact on volatility saved to outputs/volatility_by_sentiment.csv\n",
      "Visualizations saved to outputs/*.png\n",
      "\n",
      "Merged file saved to: outputs/merged_trades_with_sentiment.csv\n",
      "\n",
      "Analysis complete. Outputs stored in the 'outputs' folder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import json, os, warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\", font_scale=1.05)\n",
    "OUT = \"outputs\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------------------------------------------\n",
    "fg = pd.read_csv(\"fear_greed_index.csv\")\n",
    "trades = pd.read_csv(\"historical_data.csv\")\n",
    "print(\"Loaded shapes FG, trades:\", fg.shape, trades.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# FEAR-GREED CLEANING\n",
    "# ---------------------------------------------------------------\n",
    "fg.rename(columns={'value': 'index_value'}, inplace=True)\n",
    "\n",
    "def parse_fg_timestamp(series):\n",
    "    s = pd.to_numeric(series, errors='coerce')\n",
    "    unit = 's' if s.median() < 1e11 else 'ms'\n",
    "    return pd.to_datetime(s, unit=unit, errors='coerce', utc=True)\n",
    "\n",
    "fg['timestamp'] = parse_fg_timestamp(fg['timestamp'])\n",
    "fg = fg.dropna(subset=['timestamp'])\n",
    "fg = fg[['timestamp', 'index_value', 'classification']].drop_duplicates()\n",
    "print(f\"FG cleaned rows: {len(fg)}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# TRADES CLEANING\n",
    "# ---------------------------------------------------------------\n",
    "ts_cols = [c for c in trades.columns if 'timestamp' in c.lower()]\n",
    "print(\"Candidate timestamp columns found:\", ts_cols)\n",
    "\n",
    "best_ts = None\n",
    "max_unique = 0\n",
    "for c in ts_cols:\n",
    "    unique_days = pd.to_datetime(trades[c], errors='coerce').dt.date.nunique()\n",
    "    print(f\"→ {c}: {unique_days} unique trading days\")\n",
    "    if unique_days > max_unique:\n",
    "        best_ts, max_unique = c, unique_days\n",
    "\n",
    "ts_col = best_ts or 'timestamp updated'\n",
    "print(f\"Best timestamp column selected: {ts_col} ({max_unique} unique days)\")\n",
    "\n",
    "# --- FIXED robust IST parsing ---\n",
    "def smart_parse_timestamp(series):\n",
    "    s = pd.to_datetime(series, errors='coerce', utc=False, infer_datetime_format=True)\n",
    "\n",
    "    if s.isna().mean() > 0.3:\n",
    "        # Try multiple common formats\n",
    "        for fmt in [\n",
    "            \"%d-%m-%Y %H:%M:%S\", \"%d/%m/%Y %H:%M:%S\",\n",
    "            \"%Y-%m-%d %H:%M:%S\", \"%d-%m-%Y %I:%M:%S %p\",\n",
    "            \"%d-%m-%Y\", \"%Y/%m/%d %H:%M\"\n",
    "        ]:\n",
    "            s2 = pd.to_datetime(series, format=fmt, errors=\"coerce\")\n",
    "            if s2.notna().sum() > s.notna().sum():\n",
    "                s = s2\n",
    "\n",
    "    # Localize safely without `errors`\n",
    "    try:\n",
    "        s = s.dt.tz_localize(\"Asia/Kolkata\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "    except Exception:\n",
    "        # If already tz-aware, skip localization\n",
    "        pass\n",
    "\n",
    "    return s.dt.tz_convert(\"UTC\")\n",
    "\n",
    "    # Localize safely without `errors`\n",
    "    try:\n",
    "        s = s.dt.tz_localize(\"Asia/Kolkata\", nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "    except Exception:\n",
    "        # If already tz-aware, skip localization\n",
    "        pass\n",
    "\n",
    "    return s.dt.tz_convert(\"UTC\")\n",
    "\n",
    "    # Localize to IST safely\n",
    "    s = s.dt.tz_localize(\"Asia/Kolkata\", nonexistent=\"shift_forward\", ambiguous=\"NaT\", errors=\"coerce\")\n",
    "    return s.dt.tz_convert(\"UTC\")\n",
    "\n",
    "trades['timestamp_final'] = smart_parse_timestamp(trades[ts_col])\n",
    "trades = trades.dropna(subset=['timestamp_final'])\n",
    "\n",
    "for c in ['Execution Price','Size Tokens','Size USD','Closed PnL','Fee']:\n",
    "    if c in trades.columns:\n",
    "        trades[c] = pd.to_numeric(trades[c], errors='coerce')\n",
    "\n",
    "print(f\"Trade timestamp range: {trades['timestamp_final'].min()} → {trades['timestamp_final'].max()}\")\n",
    "print(f\"FG timestamp range: {fg['timestamp'].min()} → {fg['timestamp'].max()}\")\n",
    "print(f\"Unique trade days detected: {trades['timestamp_final'].dt.date.nunique()}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# NEAREST TIMESTAMP MERGE\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nMerging each trade with nearest Fear-Greed timestamp (within 1 day)...\")\n",
    "\n",
    "trades = trades.sort_values('timestamp_final')\n",
    "fg = fg.sort_values('timestamp')\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    trades, fg,\n",
    "    left_on='timestamp_final',\n",
    "    right_on='timestamp',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('1D')\n",
    ")\n",
    "\n",
    "merged = merged.dropna(subset=['index_value', 'classification'])\n",
    "print(f\"Final merged rows: {len(merged)}\")\n",
    "print(f\"Merged timestamp range: {merged['timestamp_final'].min()} → {merged['timestamp_final'].max()}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# SENTIMENT SUMMARY\n",
    "# ---------------------------------------------------------------\n",
    "sent_summary = merged.groupby('classification').agg(\n",
    "    trades=('timestamp_final','count'),\n",
    "    avg_pnl=('Closed PnL','mean'),\n",
    "    median_pnl=('Closed PnL','median'),\n",
    "    avg_volume=('Size USD','mean'),\n",
    "    avg_exec=('Execution Price','mean')\n",
    ").reset_index()\n",
    "\n",
    "sent_summary.to_csv(f\"{OUT}/sentiment_summary.csv\", index=False)\n",
    "print(\"\\nSentiment summary:\\n\", sent_summary)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# STATISTICAL TESTS\n",
    "# ---------------------------------------------------------------\n",
    "groups = [g['Closed PnL'].dropna() for _, g in merged.groupby('classification')]\n",
    "kw_res = stats.kruskal(*groups) if len(groups) > 1 else None\n",
    "if kw_res:\n",
    "    print(f\"\\nKruskal-Wallis p-value: {kw_res.pvalue:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# VOLATILITY ANALYSIS\n",
    "# ---------------------------------------------------------------\n",
    "merged['PnL_roll_std_500'] = merged['Closed PnL'].rolling(500, min_periods=20).std()\n",
    "vol_by_sent = merged.groupby('classification')['PnL_roll_std_500'].mean().reset_index()\n",
    "vol_by_sent.to_csv(f\"{OUT}/volatility_by_sentiment.csv\", index=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# MODEL: Predict PnL Using Sentiment\n",
    "# ---------------------------------------------------------------\n",
    "model_df = merged.dropna(subset=['Closed PnL','index_value']).copy()\n",
    "model_df['hour'] = model_df['timestamp_final'].dt.hour\n",
    "model_df = pd.get_dummies(model_df, columns=['classification'], drop_first=True)\n",
    "X = model_df.select_dtypes(include=[np.number]).drop(columns=['Closed PnL'], errors='ignore')\n",
    "y = model_df['Closed PnL']\n",
    "\n",
    "if len(X) >= 500:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    rmse = np.sqrt(np.mean((preds - y_test)**2))\n",
    "    perm = permutation_importance(rf, X_test, y_test, n_repeats=20, random_state=42)\n",
    "    imp = pd.Series(perm.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
    "    imp.head(15).to_csv(f\"{OUT}/feature_importances.csv\")\n",
    "    print(f\"\\nRandom Forest RMSE: {rmse:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNot enough data to train a robust model.\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# VISUALIZATIONS\n",
    "# ---------------------------------------------------------------\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.lineplot(x='timestamp_final', y='index_value', data=merged, color='royalblue', linewidth=1.5)\n",
    "plt.title(\"Fear-Greed Index (Nearest Timestamp Merge)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Timestamp\"); plt.ylabel(\"Index Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT}/fig_timeseries_index.png\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.lineplot(x='timestamp_final', y='Closed PnL', data=merged, color='seagreen', linewidth=1.5)\n",
    "plt.title(\"Trader Closed PnL Over Time\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Timestamp\"); plt.ylabel(\"Closed PnL\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT}/fig_timeseries_pnl.png\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.regplot(x='index_value', y='Closed PnL', data=merged, scatter_kws={'alpha':0.3}, line_kws={'color':'crimson'})\n",
    "plt.title(\"Fear-Greed Index vs Closed PnL\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT}/fig_scatter_index_vs_pnl.png\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='classification', y='Closed PnL', data=merged, palette='viridis')\n",
    "plt.title(\"PnL Distribution by Sentiment\", fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUT}/fig_box_pnl_by_sentiment.png\"); plt.close()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# INSIGHTS SUMMARY\n",
    "# ---------------------------------------------------------------\n",
    "corr_val = merged['index_value'].corr(merged['Closed PnL'])\n",
    "ins = [\n",
    "    f\"Rows merged: {len(merged)}\",\n",
    "    f\"Correlation (Fear-Greed vs Closed_PnL): {corr_val:.4f}\",\n",
    "]\n",
    "if kw_res:\n",
    "    ins.append(f\"Kruskal-Wallis p-value: {kw_res.pvalue:.4e}\")\n",
    "if len(X) >= 500:\n",
    "    ins.append(f\"Random Forest RMSE: {rmse:.3f}\")\n",
    "ins.append(\"Sentiment impact on volatility saved to outputs/volatility_by_sentiment.csv\")\n",
    "ins.append(\"Visualizations saved to outputs/*.png\")\n",
    "\n",
    "# Save insights\n",
    "open(f\"{OUT}/insights.txt\", \"w\").write(\"\\n\".join(ins))\n",
    "\n",
    "# Save merged data to CSV for verification\n",
    "merged.to_csv(f\"{OUT}/merged_trades_with_sentiment.csv\", index=False)\n",
    "\n",
    "# Confirm in console\n",
    "print(\"\\n\".join(ins))\n",
    "print(f\"\\nMerged file saved to: {OUT}/merged_trades_with_sentiment.csv\")\n",
    "print(\"\\nAnalysis complete. Outputs stored in the 'outputs' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc26b14-7443-4fd0-a29a-43cb5afb433c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
